{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서(Tensors)\n",
    "\n",
    "텐서는 배열(arrays) 및 행렬(matrix)과 매우 유사한 전문화된 데이터 구조입니다.\n",
    "파이토치에서는 텐서를 사용하여 모델의 입력 및 출력뿐만 아니라 모델의 파라미터를 인코딩합니다.\n",
    "\n",
    "텐서는 [Numpy](https://numpy.org/)의 ndarrays와 유사하지만 텐서는 GPU 또는 다른 하드웨어 가속기에서 실행할 수 있다. 실제로 텐서와 NumPy 배열은 동일한 기본 메모리를 공유할 수 있으므로 데이터를 복사할 필요가 없다(`bridge-to-np-label` 참조). 텐서는 자동 미분에 최적화되어 있습니다(이 후 Autograd 부분에서 자세히 살펴볼 예정입니다.). 당신이 ndarrays에 익숙하다면 Tensor API를 바로 사용할 수 있습니다. 따라오세요!\n",
    "\n",
    "환경 조성하는 것부터 시작합시다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서 생성\n",
    "\n",
    "텐서는 다양한 방법으로 생성할 수 있습니다. 다음 예를 살펴봅시다:\n",
    "\n",
    "\n",
    "## 데이터로부터 직접 생성\n",
    "\n",
    "텐서는 데이터에서 직접 만들 수 있습니다. 데이터의 유형은 자동으로 유추됩니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy 배열에서부터 생성\n",
    "\n",
    "Tensors can be created from NumPy arrays (and vice versa - see `bridge-to-np-label`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다른 텐서로부터 생성:\n",
    "\n",
    "새로운 텐서는 명시적으로 재정의되지 않는 한 텐서의 속성(모양, 데이터 타입)을 유지한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 무작위 또는 상수 값을 사용하여 생성:\n",
    "\n",
    "``모양(Shape)``은 텐서 차원의 튜플(tuple)입니다. 아래 함수에서 출력 텐서의 차원을 결정합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서의 특성\n",
    "\n",
    "텐서의 특성은 모양, 데이터 타입 및 텐서가 저장되는 장치를 설명합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서 연산\n",
    "\n",
    "산술, 선형 대수, 행렬 조작(전치(transposing), 인덱싱, 슬라이스), 샘플링 등 100개 이상의 텐서 연산이 있으며\n",
    "[여기](https://pytorch.org/docs/stable/torch.html)에 더욱 자세하게 기술되어 있다.\n",
    "\n",
    "이러한 각 작업은 GPU(일반적으로 CPU보다 빠른 속도)에서 실행할 수 있습니다.\n",
    "\n",
    "기본적으로 텐서는 CPU에서 생성됩니다. 우리는 GPU의 가용성을 확인한 후, `.to` 메서드를 사용하여 \n",
    "텐서를 GPU로 명시적으로 이동시켜야 한다. 장치 간에 대규모의 텐서를 복사하는 것은 시간과 메모리 측면에서\n",
    "비용이 많이 들 수 있다는 점을 명심하십시오!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "목록에서 몇 가지 작업을 시도해 보십시오. NumPy API를 잘 알고 있으면 Tensor API를 쉽게 사용할 수 있습니다.\n",
    "\n",
    "## 표준 numpy 방식의 인덱싱과 슬라이싱:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print('First row: ',tensor[0])\n",
    "print('First column: ', tensor[:, 0])\n",
    "print('Last column:', tensor[..., -1])\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 결합\n",
    "\n",
    "`torch.cat`을 사용하여 주어진 차원을 따라 일련의 텐서를 결합할 수 있습니다.\n",
    "``torch.cat``과 약간 다른 텐서 결합 메서드인 [torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html)을 참조하십시오,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 산술 연산\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "\n",
    "\n",
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단일 요소 텐서\n",
    "\n",
    "예를 들어 텐서의 모든 값을 하나의 값으로 집계하여 one-element 텐서가 있는 경우, `item()`을 사용하여 이를 Python numerial 값으로 변환할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()  \n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인플레이스(In-place) 연산\n",
    "\n",
    "결과를 피연산자에 저장하는 연산을 In-place라고 합니다. 그들은 ``_`` 접미사에 의해 표시됩니다.\n",
    "예를 들어 : ``x.copy_(y)``, ``x.t_()``는 ``x``가 바뀌게 될 것 입니다.\n",
    "\n",
    "\n",
    "> **Note:** 인플레이스 연산은 약간의 메모리를 절약하지만, 기록이 즉시 손실되기 때문에 파생 항목을 계산할 때 문제가 될 수 있어 사용하지 않는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bridge with NumPy\n",
    "\n",
    "CPU 및 NumPy Array의 Tensor는 기본 메모리 위치를 공유 할 수 있으며 하나를 변경하면 나머지도 같이 변경됩니다.\n",
    "\n",
    "### Tensor to NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor의 변경은 NumPy Array에 반영됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy array to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy Array의 변경은 Tensor에 반영됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "conda-env-py37_pytorch-py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
